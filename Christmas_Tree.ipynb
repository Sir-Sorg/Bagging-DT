{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Ensembles - Bagging \n",
    "*Ensemble methods*, which combines several decision trees to produce better predictive performance than utilizing a single decision tree. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "Import the Following Libraries:\n",
    "- csv\n",
    "- numpy (as np)\n",
    "- DecisionTreeClassifier from sklearn.tree\n",
    "- preprocessing from sklearn\n",
    "- train_test_split from sklearn\n",
    "- classification_report from sklearn\n",
    "- matplotlib (as plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "Now, ***read the data*** using *csv*\n",
    "\n",
    "The following functions **readData** will read data from csv file And returns all the data in the dimensions of the file itself <br>\n",
    "Then in the next step, we prepare it for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(address):\n",
    "    with open(address) as csvFile:\n",
    "        reader = csv.reader(csvFile)\n",
    "        data = [row for row in reader]\n",
    "    return data\n",
    "\n",
    "\n",
    "def cleanData(data):\n",
    "    return list(filter(lambda thisList: False if '?' in thisList else True, data))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the information and store the number of available data in a variable, then in the next stage of cleaning, we reduce the size of the information from the initial amount to find the number of rows containing *missing values* and print **percentage** of this Incorrect information.\n",
    "<br>\n",
    "\n",
    "Remove the row containing the headers name since it doesn't contain any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileAddress = './train+dev+test.csv'\n",
    "data = readData(fileAddress)\n",
    "missingValues = len(data)\n",
    "print(f\"Number of rows before data cleaning: {len(data)}\")\n",
    "data = cleanData(data)\n",
    "missingValues -= len(data)\n",
    "data = np.array(data)[1:]  # remove headers\n",
    "print(f\"Number of rows after data cleaning: {len(data)}\")\n",
    "missingValues = round(missingValues/(len(data)+missingValues)*100, 2)\n",
    "print(f\"Percentage of missing values: {missingValues}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicator variables\n",
    "As you may figure out, All features in this dataset are categorical, such as **cap-shape** or **habitat**. Sklearn Decision Trees does not handle categorical variables. We can still convert these features to numerical values using `dummyVariables` to convert the categorical variable into dummy/indicator variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummyVariables(features):\n",
    "    for column in range(features.shape[1]):\n",
    "        # 0,1,2,3,...,21\n",
    "        featureStatus = set(features[:, column])\n",
    "        tranasformer = preprocessing.LabelEncoder()\n",
    "        tranasformer.fit(list(featureStatus))\n",
    "        features[:, column] = tranasformer.transform(features[:, column])\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now separate the labels of the samples and their features:\n",
    "- **X** as the Feature Matrix (data)\n",
    "- **Y** as the response vector (target)\n",
    "<br>\n",
    "\n",
    "Then we give the list of features **X** to the number converter function `dummyVariables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 1:]\n",
    "Y = data[:, 0]\n",
    "print(f\"Before indicator variables: \\n{X}\")\n",
    "X = dummyVariables(X)\n",
    "print(f\"\\nAfter indicator variables: \\n{X}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test split\n",
    "I using train/test split to train and test decision tree,\n",
    "train_test_split will return 4 different parameters. We will name them:\n",
    "`X_train, X_test, y_train, y_test`.\n",
    "\n",
    "The X and y are the arrays required before the split, the test_size represents the ratio of the testing dataset, and the random_state ensures that we obtain the same splits.\n",
    "\n",
    "I chose the ratio of train and test set 70% and 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=24)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap\n",
    "Now, in order to train ***k*** numbers of classifiers, we also need ***k*** numbers of training sets, that's why we extract new training data from the training data sets ***k*** times.\n",
    "\n",
    "For this purpose, I use the `bootstrap` function, which takes the set of features and labels and arranges them, then it stores the same amount of data from them as the first set, using placement, and then separates the label from the feature and return two collection provides for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, Y):\n",
    "    dataset = np.column_stack((X, Y))\n",
    "    newDataset = dataset[np.random.choice(\n",
    "        dataset.shape[0], size=dataset.shape[0])]\n",
    "    new_X = newDataset[:, :-1]\n",
    "    new_Y = newDataset[:, -1]\n",
    "    return new_X, new_Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform the above function 5 times and a list is obtained that containing 5 training datasets, each of which is a **tuple** of **feature** and **label** pairs.\n",
    "<br>\n",
    "\n",
    "Below we print an example of that pair that will be used to train the last classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_BOOTSTRAP = 5\n",
    "bootstrapDataset = [bootstrap(X_train, y_train)\n",
    "                    for _ in range(NUMBER_OF_BOOTSTRAP)]\n",
    "print(f\"A pair including features and labels: \\n{bootstrapDataset[-1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifire\n",
    "We will first create an instance of the **DecisionTreeClassifier** called **tree**.\n",
    "Inside of the classifier, specify criterion=\"entropy\" so we can see the information gain of each node.\n",
    "\n",
    "Next, we will fit the data with the training feature `bootstrapDataset` and training response vector\n",
    "I add this generated tree to a list, and repeat this cycle 5 times until 5 trees are formed from 5 series of training datasets.\n",
    "\n",
    "In the last line, as an example, I print the type of the variable in the last cell of the `classifires` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "for index in range(NUMBER_OF_BOOTSTRAP):\n",
    "    # Define Decision Thee\n",
    "    tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)\n",
    "    tree.fit(*bootstrapDataset[index])\n",
    "    classifiers.append(tree)\n",
    "print(f\"The last classifier is: {type(classifiers[-1])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "make some predictions for each tree on the testing dataset and store it into a list called `votes`. \n",
    "<br>\n",
    "Currently, this list contains 5 rows and n columns, which represent the opinion of each tree about the test samples\n",
    "In order to be clean and convenient in calculations, we convert it into a Matrix containing n rows and 5 columns, where each row represents the opinion of the trees about that test sample, and n is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = [tree.predict(X_test) for tree in classifiers]\n",
    "votes = np.array(votes)\n",
    "print(f\"Dimensions of opinion before reshaping: {votes.shape}\")\n",
    "votes = np.transpose(votes)\n",
    "print(f\"Dimensions of opinion after reshaping: {votes.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting\n",
    "Now, for each example of the tests, we walk on the Matrix of opinions, and the vote that has the most repetition is used as the main label and stored in the list of predictions `predicted_Y`.\n",
    "<br>\n",
    "By going through the list of votes, each cell contains a list of 5 vote, which is the example, which is given to the `majority` function and returns the common vote.\n",
    "<br>\n",
    "Finally, the total number of Consensus opinion are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority(vote):\n",
    "    vote = list(vote)\n",
    "    return max(set(vote), key=vote.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_Y = [majority(vote) for vote in votes]\n",
    "print(f\"Number of Consensus votes: {len(predicted_Y)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Accuracy classification score computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding real labels **y_test**.\n",
    "\n",
    "In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = classification_report(y_test, predicted_Y)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
