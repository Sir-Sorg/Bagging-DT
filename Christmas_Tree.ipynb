{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Ensembles - Bagging \n",
    "*Ensemble methods*, which combines several decision trees to produce better predictive performance than utilizing a single decision tree. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Following Libraries:\n",
    "- csv\n",
    "- numpy (as np)\n",
    "- DecisionTreeClassifier from sklearn.tree\n",
    "- preprocessing from sklearn\n",
    "- train_test_split from sklearn\n",
    "- classification_report from sklearn\n",
    "- matplotlib (as plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ***read the data*** using *csv*\n",
    "\n",
    "The following functions **readData** will read data from csv file And returns all the data in the dimensions of the file itself <br>\n",
    "Then in the next step, we prepare it for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(address):\n",
    "    with open(address) as csvFile:\n",
    "        reader = csv.reader(csvFile)\n",
    "        data = [row for row in reader]\n",
    "    return data\n",
    "\n",
    "\n",
    "def cleanData(data):\n",
    "    return list(filter(lambda thisList: False if '?' in thisList else True, data))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the information and store the number of available data in a variable, then in the next stage of cleaning, we reduce the size of the information from the initial amount to find the number of rows containing *missing values* and print **percentage** of this Incorrect information.\n",
    "<br>\n",
    "\n",
    "Remove the row containing the headers name since it doesn't contain any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before data cleaning: 8125\n",
      "Number of rows after data cleaning: 5644\n",
      "Percentage of missing values: 30.53%\n"
     ]
    }
   ],
   "source": [
    "fileAddress = './train+dev+test.csv'\n",
    "data = readData(fileAddress)\n",
    "missingValues = len(data)\n",
    "print(f\"Number of rows before data cleaning: {len(data)}\")\n",
    "data = cleanData(data)\n",
    "missingValues -= len(data)\n",
    "data = np.array(data)[1:]  # remove headers\n",
    "print(f\"Number of rows after data cleaning: {len(data)}\")\n",
    "missingValues = round(missingValues/(len(data)+missingValues)*100, 2)\n",
    "print(f\"Percentage of missing values: {missingValues}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may figure out, All features in this dataset are categorical, such as **cap-shape** or **habitat**. Sklearn Decision Trees does not handle categorical variables. We can still convert these features to numerical values using `dummyVariables` to convert the categorical variable into dummy/indicator variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummyVariables(features):\n",
    "    for column in range(features.shape[1]):\n",
    "        # 0,1,2,3,...,21\n",
    "        featureStatus = set(features[:, column])\n",
    "        tranasformer = preprocessing.LabelEncoder()\n",
    "        tranasformer.fit(list(featureStatus))\n",
    "        features[:, column] = tranasformer.transform(features[:, column])\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now separate the labels of the samples and their features:\n",
    "- **X** as the Feature Matrix (data)\n",
    "- **Y** as the response vector (target)\n",
    "<br>\n",
    "\n",
    "Then we give the list of features **X** to the number converter function `dummyVariables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before indicator variables: \n",
      "[['x' 's' 'n' ... 'k' 's' 'u']\n",
      " ['x' 's' 'y' ... 'n' 'n' 'g']\n",
      " ['b' 's' 'w' ... 'n' 'n' 'm']\n",
      " ...\n",
      " ['x' 'y' 'g' ... 'w' 'y' 'p']\n",
      " ['x' 'y' 'c' ... 'w' 'c' 'd']\n",
      " ['f' 'y' 'c' ... 'w' 'c' 'd']]\n",
      "\n",
      "After indicator variables: \n",
      "[['5' '2' '4' ... '1' '3' '5']\n",
      " ['5' '2' '7' ... '2' '2' '1']\n",
      " ['0' '2' '6' ... '2' '2' '3']\n",
      " ...\n",
      " ['5' '3' '3' ... '5' '5' '4']\n",
      " ['5' '3' '1' ... '5' '1' '0']\n",
      " ['2' '3' '1' ... '5' '1' '0']]\n"
     ]
    }
   ],
   "source": [
    "X = data[:, 1:]\n",
    "Y = data[:, 0]\n",
    "print(f\"Before indicator variables: \\n{X}\")\n",
    "X = dummyVariables(X)\n",
    "print(f\"\\nAfter indicator variables: \\n{X}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I using train/test split to train and test decision tree,\n",
    "train_test_split will return 4 different parameters. We will name them:\n",
    "`X_train, X_test, y_train, y_test`.\n",
    "\n",
    "The X and y are the arrays required before the split, the test_size represents the ratio of the testing dataset, and the random_state ensures that we obtain the same splits.\n",
    "\n",
    "I chose the ratio of train and test set 70% and 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=24)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to train ***k*** numbers of classifiers, we also need ***k*** numbers of training sets, that's why we extract new training data from the training data sets ***k*** times.\n",
    "\n",
    "For this purpose, I use the `bootstrap` function, which takes the set of features and labels and arranges them, then it stores the same amount of data from them as the first set, using placement, and then separates the label from the feature and return two collection provides for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, Y):\n",
    "    dataset = np.column_stack((X, Y))\n",
    "    newDataset = dataset[np.random.choice(\n",
    "        dataset.shape[0], size=dataset.shape[0])]\n",
    "    new_X = newDataset[:, :-1]\n",
    "    new_Y = newDataset[:, -1]\n",
    "    return new_X, new_Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform the above function 5 times and a list is obtained that containing 5 training datasets, each of which is a **tuple** of **feature** and **label** pairs.\n",
    "<br>\n",
    "\n",
    "Below we print an example of that pair that will be used to train the last classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A pair including features and labels: \n",
      "(array([['0', '3', '7', ..., '1', '3', '1'],\n",
      "       ['2', '3', '7', ..., '2', '5', '1'],\n",
      "       ['2', '3', '3', ..., '1', '4', '0'],\n",
      "       ...,\n",
      "       ['5', '2', '6', ..., '1', '3', '1'],\n",
      "       ['2', '0', '4', ..., '2', '3', '1'],\n",
      "       ['5', '3', '3', ..., '0', '4', '4']], dtype='<U24'), array(['e', 'e', 'e', ..., 'e', 'e', 'p'], dtype='<U24'))\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_BOOTSTRAP = 5\n",
    "bootstrapDataset = [bootstrap(X_train, y_train)\n",
    "                    for _ in range(NUMBER_OF_BOOTSTRAP)]\n",
    "print(f\"A pair including features and labels: \\n{bootstrapDataset[-1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifire\n",
    "We will first create an instance of the **DecisionTreeClassifier** called **tree**.\n",
    "Inside of the classifier, specify criterion=\"entropy\" so we can see the information gain of each node.\n",
    "\n",
    "Next, we will fit the data with the training feature `bootstrapDataset` and training response vector\n",
    "I add this generated tree to a list, and repeat this cycle 5 times until 5 trees are formed from 5 series of training datasets.\n",
    "\n",
    "In the last line, as an example, I print the type of the variable in the last cell of the `classifires` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last classifier is: <class 'sklearn.tree._classes.DecisionTreeClassifier'>\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "for index in range(NUMBER_OF_BOOTSTRAP):\n",
    "    # Define Decision Thee\n",
    "    tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)\n",
    "    tree.fit(*bootstrapDataset[index])\n",
    "    classifiers.append(tree)\n",
    "print(f\"The last classifier is: {type(classifiers[-1])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = [tree.predict(X_test) for tree in classifiers]\n",
    "votes = np.array(votes)\n",
    "votes = np.transpose(votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority(vote):\n",
    "    # find most frequent element in a list\n",
    "    vote = list(vote)\n",
    "    return max(set(vote), key=vote.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the majority vote\n",
    "predicted_Y = [majority(vote) for vote in votes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           e       1.00      0.99      0.99      1083\n",
      "           p       0.98      0.99      0.98       611\n",
      "\n",
      "    accuracy                           0.99      1694\n",
      "   macro avg       0.99      0.99      0.99      1694\n",
      "weighted avg       0.99      0.99      0.99      1694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# figure out my tree accuracy\n",
    "accuracy = classification_report(y_test, predicted_Y)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
