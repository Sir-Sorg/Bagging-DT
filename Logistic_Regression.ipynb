{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "To estimate the class of a data point, we need some sort of guidance on what would be the most probable class for that data point. For this, we use Logistic Regression.\n",
    "Logistic regression fits a special s-shaped curve by taking the linear regression function and transforming the numeric estimate into a probability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "Import the Following Libraries:\n",
    "- csv\n",
    "- numpy (as np)\n",
    "- LogisticRegression from sklearn.linear_model\n",
    "- preprocessing from sklearn\n",
    "- train_test_split from sklearn\n",
    "- classification_report from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "Now, ***read the data*** using *csv*\n",
    "\n",
    "The following functions **readData** will read data from csv file And returns all the data in the dimensions of the file itself <br>\n",
    "Then in the next step, we prepare it for pre-processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(address):\n",
    "    with open(address) as csvFile:\n",
    "        reader = csv.reader(csvFile)\n",
    "        data = [row for row in reader]\n",
    "    return data\n",
    "\n",
    "\n",
    "def cleanData(data):\n",
    "    return list(filter(lambda thisList: False if '?' in thisList else True, data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the information and store the number of available data in a variable, then in the next stage of cleaning, we reduce the size of the information from the initial amount to find the number of rows containing *missing values* and print **percentage** of this Incorrect information.\n",
    "<br>\n",
    "\n",
    "Remove the row containing the headers name since it doesn't contain any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before data cleaning: 8125\n",
      "Number of rows after data cleaning: 5644\n",
      "Percentage of missing values: 30.53%\n"
     ]
    }
   ],
   "source": [
    "fileAddress = './train+dev+test.csv'\n",
    "data = readData(fileAddress)\n",
    "missingValues = len(data)\n",
    "print(f\"Number of rows before data cleaning: {len(data)}\")\n",
    "data = cleanData(data)\n",
    "missingValues -= len(data)\n",
    "data = np.array(data)[1:]  # remove headers\n",
    "print(f\"Number of rows after data cleaning: {len(data)}\")\n",
    "missingValues = round(missingValues/(len(data)+missingValues)*100, 2)\n",
    "print(f\"Percentage of missing values: {missingValues}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicator variables\n",
    "As you may figure out, All features in this dataset are categorical, such as **cap-shape** or **habitat**. Sklearn Logestic Regression does not handle categorical variables. We can still convert these features to numerical values using `dummyVariables` to convert the categorical variable into dummy/indicator variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummyVariables(features):\n",
    "    for column in range(features.shape[1]):\n",
    "        # 0,1,2,3,...,21\n",
    "        featureStatus = set(features[:, column])\n",
    "        tranasformer = preprocessing.LabelEncoder()\n",
    "        tranasformer.fit(list(featureStatus))\n",
    "        features[:, column] = tranasformer.transform(features[:, column])\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now separate the labels of the samples and their features:\n",
    "- **X** as the Feature Matrix (data)\n",
    "- **Y** as the response vector (target)\n",
    "<br>\n",
    "\n",
    "Then we give the list of features **X** to the number converter function `dummyVariables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before indicator variables: \n",
      "[['x' 's' 'n' ... 'k' 's' 'u']\n",
      " ['x' 's' 'y' ... 'n' 'n' 'g']\n",
      " ['b' 's' 'w' ... 'n' 'n' 'm']\n",
      " ...\n",
      " ['x' 'y' 'g' ... 'w' 'y' 'p']\n",
      " ['x' 'y' 'c' ... 'w' 'c' 'd']\n",
      " ['f' 'y' 'c' ... 'w' 'c' 'd']]\n",
      "\n",
      "After indicator variables: \n",
      "[['5' '2' '4' ... '1' '3' '5']\n",
      " ['5' '2' '7' ... '2' '2' '1']\n",
      " ['0' '2' '6' ... '2' '2' '3']\n",
      " ...\n",
      " ['5' '3' '3' ... '5' '5' '4']\n",
      " ['5' '3' '1' ... '5' '1' '0']\n",
      " ['2' '3' '1' ... '5' '1' '0']]\n"
     ]
    }
   ],
   "source": [
    "X = data[:, 1:]\n",
    "Y = data[:, 0]\n",
    "print(f\"Before indicator variables: \\n{X}\")\n",
    "X = dummyVariables(X)\n",
    "print(f\"\\nAfter indicator variables: \\n{X}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data\n",
    "Data Standardization gives the data zero mean and unit variance, it is good practice, especially for algorithms such as KNN, Logestic Regression, ... which is based on the Coordinates of data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(data):\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    STD_data = scaler.transform(data)\n",
    "    return STD_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after Normalization: \n",
      "[[ 0.95193532  0.27895188 -0.14846445 ... -0.20344067 -0.53662286\n",
      "   2.35567928]\n",
      " [ 0.95193532  0.27895188  1.48387322 ...  0.7408185  -1.28928177\n",
      "  -0.14770122]\n",
      " [-2.06103179  0.27895188  0.93976067 ...  0.7408185  -1.28928177\n",
      "   1.10398903]\n",
      " ...\n",
      " [ 0.95193532  1.02724296 -0.692577   ...  3.573596    0.96869495\n",
      "   1.72983415]\n",
      " [ 0.95193532  1.02724296 -1.78080212 ...  3.573596   -2.04194068\n",
      "  -0.77354635]\n",
      " [-0.85584494  1.02724296 -1.78080212 ...  3.573596   -2.04194068\n",
      "  -0.77354635]]\n"
     ]
    }
   ],
   "source": [
    "X = standardization(X)\n",
    "print(f\"Data after Normalization: \\n{X}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test split\n",
    "I using train/test split to train and test decision tree,\n",
    "train_test_split will return 4 different parameters. We will name them:\n",
    "`X_train, X_test, y_train, y_test`.\n",
    "\n",
    "The X and y are the arrays required before the split, the test_size represents the ratio of the testing dataset, and the random_state ensures that we obtain the same splits.\n",
    "\n",
    "I chose the ratio of train and test set 70% and 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (3950, 22) (3950,)\n",
      "Test set: (1694, 22) (1694,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=24)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "Let's build our model using LogisticRegression from the Scikit-learn package. This function implements logistic regression and can use different numerical optimizers to find parameters, including *‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’* solvers.\n",
    "\n",
    "The version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem of machine learning models. Now let's fit our model with train set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Logestic Regression classifier is: LogisticRegression(solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "logisticRegression = LogisticRegression(solver='liblinear')\n",
    "logisticRegression.fit(X_train, y_train)\n",
    "print(f\"The Logestic Regression classifier is: {logisticRegression}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "We can use the model to make predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated class is: ['e' 'p' 'e' ... 'e' 'e' 'e']\n"
     ]
    }
   ],
   "source": [
    "yhat = logisticRegression.predict(X_test)\n",
    "print(f'Estimated class is: {yhat}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Probability\n",
    "predict_proba returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 0, P(Y=0|X), and second column is probability of class 1, P(Y=1|X):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Probability of classes: \n",
      "[[1.    0.   ]\n",
      " [0.001 0.999]\n",
      " [1.    0.   ]\n",
      " ...\n",
      " [1.    0.   ]\n",
      " [0.768 0.232]\n",
      " [1.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "predictedProbability = logisticRegression.predict_proba(X_test)\n",
    "predictedProbability = np.around(predictedProbability, decimals=3)\n",
    "print(f'The Probability of classes: \\n{predictedProbability}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Accuracy classification score computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding real labels **y_test**.\n",
    "\n",
    "In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           e       0.98      0.99      0.99      1083\n",
      "           p       0.98      0.97      0.98       611\n",
      "\n",
      "    accuracy                           0.98      1694\n",
      "   macro avg       0.98      0.98      0.98      1694\n",
      "weighted avg       0.98      0.98      0.98      1694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
